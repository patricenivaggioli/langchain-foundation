{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51d58587",
   "metadata": {},
   "source": [
    "# Module 1 - Foundational Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb8e142",
   "metadata": {},
   "source": [
    "This introductory module serves as a comprehensive primer on building functional AI agents using LangChain, moving systematically from basic initialization to complex multi-modal interactions. You will begin by learning to configure chat models and refine their behavior through system prompts and engineering techniques, before progressing to the critical step of integrating external tools to expand the agent's utility. The lab also covers the implementation of short-term memory for fluid conversations and the use of audio and visual inputs to create a more versatile assistant. Ultimately, the course culminates in a practical project where you will synthesize your skills to develop a web-connected culinary assistant capable of generating recipes from specific ingredients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7809b0d7",
   "metadata": {},
   "source": [
    "## 00. Getting started"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8700b0",
   "metadata": {},
   "source": [
    "Load the required libraries and environment variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5951053",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp /home/ubuntu/.env_vars .env\n",
    "!pip install dotenv\n",
    "!pip install langchain-openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106cf38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810e9c4a",
   "metadata": {},
   "source": [
    "## 01. Initialising and invoking a model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fddb7e6",
   "metadata": {},
   "source": [
    "The foundation of any agent is the chat model, described as the conductor of the orchestra or the system's \"thinking brain\". The interaction with models should be straightforward and interchangeable.\n",
    "Use the `AzureChatOpenAI` function to select a model (e.g., gpt-4o).\n",
    "\n",
    "In our lab, the model has been defined using the `AZURE_OPENAI_DEPLOYMENT` environment variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f296205-d2e6-436e-ade4-ca3f5e0f4a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo $AZURE_OPENAI_DEPLOYMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c3ed29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "model = AzureChatOpenAI(\n",
    "    azure_deployment=os.getenv(\"AZURE_OPENAI_DEPLOYMENT\"),\n",
    "    api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e876aa4",
   "metadata": {},
   "source": [
    "Then, you interact with the model by invoking it with a question. The response is returned as an AI message object, where the primary answer is stored within the content key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf3ad14",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = model.invoke(\"What's the capital of the Moon?\")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee8e957",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673a54e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(response.response_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf169b71",
   "metadata": {},
   "source": [
    "## 02. Customising your Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7ea42c",
   "metadata": {},
   "source": [
    "Beyond the raw text response, models provide metadata (like token usage) and can be fine-tuned via specific parameters to control their behavior.  \n",
    "• Temperature: Controls randomness; higher numbers increase creativity, while lower numbers make the model more deterministic.  \n",
    "• Max Tokens: Limits the length of the output.  \n",
    "• Timeout & Retries: Timeout sets the maximum wait time for a response, and max_retries defines how many times the system should attempt a failed request.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f50c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "model = AzureChatOpenAI(\n",
    "    azure_deployment=os.getenv(\"AZURE_OPENAI_DEPLOYMENT\"),\n",
    "    api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
    "    temperature=1.0\n",
    ")\n",
    "\n",
    "response = model.invoke(\"What's the capital of the Moon?\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c14f450",
   "metadata": {},
   "source": [
    "## 03. Model Providers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb23e765",
   "metadata": {},
   "source": [
    "A key strength of LangChain is that it is model agnostic, meaning you can switch between different providers (OpenAI, Claude, Gemini) with minimal code changes.\n",
    "You can swap an entire agent's \"brain\" by changing the model name in the initialization function or using specific provider libraries for newer models. Please refer to the following documentation to get details on supported providers: https://docs.langchain.com/oss/python/integrations/chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece1cfc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "model = AzureChatOpenAI(\n",
    "    azure_deployment=\"gpt-5.1\",\n",
    "    api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
    ")\n",
    "\n",
    "response = model.invoke(\"What's the capital of the Moon?\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc723f48",
   "metadata": {},
   "source": [
    "## 04. Initialising and invoking an agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e50b4e",
   "metadata": {},
   "source": [
    "To move from a simple chat model to an agent, LangChain uses an abstraction built on top of LangGraph. The agent is initialized by passing the model as the primary argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbc17be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain.agents import create_agent\n",
    "\n",
    "model = AzureChatOpenAI(\n",
    "    azure_deployment=os.getenv(\"AZURE_OPENAI_DEPLOYMENT\"),\n",
    "    api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
    ")\n",
    "\n",
    "agent = create_agent(model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c67f56f",
   "metadata": {},
   "source": [
    "Then, when invoking the agent, you pass a dictionary containing a human message function `Human Communication` to specify the user's input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5251725",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.messages import HumanMessage\n",
    "\n",
    "response = agent.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"What's the capital of the Moon?\")]}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8438094b",
   "metadata": {},
   "source": [
    "Unlike a simple model call, the agent returns a dictionary of messages representing the full conversation history; the final answer is always the last message in that list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac95763",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50740b4f",
   "metadata": {},
   "source": [
    "In the next command, response['messages'][-1] selects the final message in the agent’s message list (the latest turn), and .content accesses that message’s text; print(...) outputs that text to the notebook’s stdout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a8e6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2a7ccb",
   "metadata": {},
   "source": [
    "Because the system accepts a dictionary, you can pass a list of previous Human and AI messages to provide context or \"gaslight\" the model with prior (even if false) information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5da573",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.messages import AIMessage\n",
    "\n",
    "response = agent.invoke(\n",
    "    {\"messages\": [HumanMessage(content=\"What's the capital of the Moon?\"),\n",
    "    AIMessage(content=\"The capital of the Moon is Luna City.\"),\n",
    "    HumanMessage(content=\"Interesting, tell me more about Luna City\")]}\n",
    ")\n",
    "\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcf635e",
   "metadata": {},
   "source": [
    "## 05. Streaming Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc618af",
   "metadata": {},
   "source": [
    "As agent systems grow complex, they must handle multiple messages and the \"perceived latency\" of slow response times. Agent response times can jump from milliseconds to seconds or minutes. To improve the user experience, use streaming tokens (printing text as it is generated) rather than waiting for the full answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7075c1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token, metadata in agent.stream(\n",
    "    {\"messages\": [HumanMessage(content=\"Tell me all about Luna City, the capital of the Moon\")]},\n",
    "    stream_mode=\"messages\"\n",
    "):\n",
    "\n",
    "    # token is a message chunk with token content\n",
    "    # metadata contains which node produced the token\n",
    "    \n",
    "    if token.content:  # Check if there's actual content\n",
    "        print(token.content, end=\"\", flush=True)  # Print token"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
